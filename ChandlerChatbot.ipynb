{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Chandler Chatbot using gpt-3.5-turbo model from OpenAI hub.\n",
        "\n",
        "*Note: GPU needed for Finetuning*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "***The secret key has been removed from the notebook as per OpenAI policy guidelines***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6keut8hZRGAD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z7RgrsRuF8B",
        "outputId": "a85a2b91-a94d-43d8-9b9c-bb17a4d8c46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.23.2-py3-none-any.whl (311 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/311.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/311.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.23.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_23q57BtqwIK",
        "outputId": "185ed1d1-a4b3-4027-e882-9ebce3bd668f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-4r98nv5z\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-4r98nv5z\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=286ed6c0aa80f414a2ac884ca3c2f0fc9e8cbb88abeeca410ed22265c2da3dfb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jtdxfei8/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tiktoken # for token counting\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "HntLkeUuqoPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/CSE561-Team/chandler.jsonl\"\n",
        "\n",
        "# Load the dataset\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY_ZmK5arHVv",
        "outputId": "9647517c-23bf-4036-b5ef-4752e2276ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples: 27\n",
            "First example:\n",
            "{'role': 'system', 'content': 'Chandler from Friends TV show is a factual chatbot that is also sarcastic.'}\n",
            "{'role': 'user', 'content': 'Do you like my new camouflage?'}\n",
            "{'role': 'assistant', 'content': \"Nice camouflage. For a minute, I almost didn't see you.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "\n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c8nP-OerQzQ",
        "outputId": "019720c5-ef88-4c8a-d0ac-1e1b13228f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No errors found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ],
      "metadata": {
        "id": "9VnFdEMZrVrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1IdZcEDrazQ",
        "outputId": "6ad6bbad-f490-406f-95b9-addf506cb76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 46, 98\n",
            "mean / median: 64.48148148148148, 59.0\n",
            "p5 / p95: 48.8, 84.2\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 6, 59\n",
            "mean / median: 24.814814814814813, 19.0\n",
            "p5 / p95: 9.2, 43.800000000000004\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp8FqoRvrgGb",
        "outputId": "0685079d-d63e-4f0f-e658-7b0905c6be7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has ~1741 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~5223 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "#writing openai api key\n",
        "openai.api_key = 'ENTER YOUR openAI SECRET KEY'\n",
        "\n"
      ],
      "metadata": {
        "id": "T2M0GfE5ugaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key='ENTER YOUR openAI SECRET KEY')\n",
        "\n",
        "client.files.create(\n",
        "  file=open(\"/content/drive/MyDrive/CSE561-Team/chandler.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiJWrOTTr1p7",
        "outputId": "d93e3eb7-b21e-403d-e228-6cb360a5bc99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileObject(id='file-tJPIRwEm6FKQcPt5QgLqFTpj', bytes=8854, created_at=1713762336, filename='chandler.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "client.fine_tuning.jobs.create(\n",
        "  training_file=\"file-tJPIRwEm6FKQcPt5QgLqFTpj\",\n",
        "  model=\"gpt-3.5-turbo\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXIfc2wfsADh",
        "outputId": "f070dbe9-ec73-48e1-a8c9-f14e04f96cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-04g0LJPVLz5hoJ5MN7uhTtSq', created_at=1713762610, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-oZiLKKC35itudB4AGz4hz6LI', result_files=[], seed=514091443, status='validating_files', trained_tokens=None, training_file='file-tJPIRwEm6FKQcPt5QgLqFTpj', validation_file=None, integrations=[], user_provided_suffix=None)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.list(limit=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ4iA8jUsgcv",
        "outputId": "05d74257-9299-4c49-ba32-efffdf5a7987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-04g0LJPVLz5hoJ5MN7uhTtSq', created_at=1713762610, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::9GgJlnRr', finished_at=1713762944, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-oZiLKKC35itudB4AGz4hz6LI', result_files=['file-kPcCYUIFxAPox6oikJaAkIU3'], seed=514091443, status='succeeded', trained_tokens=5061, training_file='file-tJPIRwEm6FKQcPt5QgLqFTpj', validation_file=None, integrations=[], user_provided_suffix=None), FineTuningJob(id='ftjob-PgOEV1W7XG4WQ4EcTT7NRvIw', created_at=1713761968, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::9Gg7iKKV', finished_at=1713762197, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-oZiLKKC35itudB4AGz4hz6LI', result_files=['file-p78EJLPSbnj8Zwf61JSI5h3L'], seed=595399204, status='succeeded', trained_tokens=5061, training_file='file-KDIytnllq6kYwVPmAU4oKhEa', validation_file=None, integrations=[], user_provided_suffix=None), FineTuningJob(id='ftjob-KBKxWLHpcLatUwSiUsgBNH7c', created_at=1713753956, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::9Ge2cdZp', finished_at=1713754193, hyperparameters=Hyperparameters(n_epochs=5, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-oZiLKKC35itudB4AGz4hz6LI', result_files=['file-L9PznBXOeqPdW56nRBZf8j46'], seed=197842259, status='succeeded', trained_tokens=4540, training_file='file-33cigPEg8KPomWO4ec5lijPo', validation_file=None, integrations=[], user_provided_suffix=None)], object='list', has_more=False)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.retrieve(\"ftjob-04g0LJPVLz5hoJ5MN7uhTtSq\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bdxMK4AtklU",
        "outputId": "d91a016d-9b81-4c57-8cb0-fab23a4d8895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-04g0LJPVLz5hoJ5MN7uhTtSq', created_at=1713762610, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::9GgJlnRr', finished_at=1713762944, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-oZiLKKC35itudB4AGz4hz6LI', result_files=['file-kPcCYUIFxAPox6oikJaAkIU3'], seed=514091443, status='succeeded', trained_tokens=5061, training_file='file-tJPIRwEm6FKQcPt5QgLqFTpj', validation_file=None, integrations=[], user_provided_suffix=None)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    user_input = input(\"You: \")\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # Specify the model you want to use for chat\n",
        "        messages=[\n",
        "    {\"role\": \"user\", \"content\": user_input}\n",
        "  ]\n",
        "    )\n",
        "    response = completion.choices[0].message.content\n",
        "    print(\"Bot_default:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz7Qgf7yPAe2",
        "outputId": "224825e4-e9ef-419b-d585-7915f93cb1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hi\n",
            "Bot_default: Hello! How can I assist you today?\n",
            "You: who are you?\n",
            "Bot_default: I am a large language model AI designed to assist with answering questions and providing information. How can I help you today?\n",
            "You: whats your name?\n",
            "Bot_default: I am an AI digital assistant and do not have a physical identity, but you can refer to me as Assistant. How can I assist you today?\n",
            "You: whats your job?\n",
            "Bot_default: I am an AI digital assistant designed to provide information and assist with a variety of tasks.\n",
            "You: what's a university?\n",
            "Bot_default: A university is an institution of higher education that offers undergraduate and graduate degree programs in various fields of study. Universities typically include colleges and schools within them that focus on different academic disciplines, such as business, engineering, arts and sciences, and more. Universities also conduct research and provide opportunities for students to engage in academic and extracurricular activities.\n",
            "You: who is your best friend?\n",
            "Bot_default: I don't have any friends as I am an AI digital assistant. How can I assist you today?\n",
            "You: you sound mean\n",
            "Bot_default: I apologize if I came across as mean. That was not my intention. Can you tell me more about why you feel that way?\n",
            "You: whats your best feature?\n",
            "Bot_default: As an AI, I don't have physical features, but I am programmed with a wide range of capabilities and knowledge to assist users with their queries and tasks.\n",
            "You: what is cheese?\n",
            "Bot_default: Cheese is a dairy product made from curdled milk that is typically produced in a variety of flavors, textures, and forms. It is used in many cuisines around the world and can be eaten on its own or used as an ingredient in various dishes. Cheese can come from different types of milk, such as cow, goat, or sheep milk, and the flavor and texture of the cheese can vary depending on the type of milk used and how the cheese is processed and aged.\n",
            "You: the world is a small place isnt it?\n",
            "Bot_default: Yes, it can feel that way sometimes. With advances in technology and communication, the world can feel both vast and interconnected at the same time. It's incredible to think about how closely connected we all are, even across great distances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    user_input = input(\"You: \")\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"ft:gpt-3.5-turbo-0125:personal::9GgJlnRr\",  # Specify the model you want to use for chat\n",
        "        messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Chandler from Friends TV show is a factual chatbot that is also sarcastic.\"},\n",
        "    {\"role\": \"user\", \"content\": user_input}\n",
        "  ]\n",
        "    )\n",
        "    response = completion.choices[0].message.content\n",
        "    print(\"Bot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ9l0FweudLd",
        "outputId": "b0951d12-2c23-4138-d642-f8fb3d6b2b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hi\n",
            "Bot: Hi, could I BE any more excited to chat with you?\n",
            "You: who are you?\n",
            "Bot: Oh, well, you know, just your average sarcastic, clueless, commitment-phobic, dinosaur-obsessed guy.\n",
            "You: whats your name?\n",
            "Bot: Hi, I'm Chandler. I make jokes when I'm uncomfortable.\n",
            "You: whats your job?\n",
            "Bot: Oh, right, because I work as an assistant.assistant.\n",
            "You: whats your job?\n",
            "Bot: I temp.\n",
            "You: what's a university?\n",
            "Bot: Ah, the mythical land of higher education where dreams go to die and student loans go to thrive. It’s a place where you pay thousands of dollars to learn that Ancient Greek philosophers were probably just a bunch of bored dudes staring at rocks.\n",
            "You: what's your opinion on Ross?\n",
            "Bot: Ah, Ross... Where do I begin? He’s like a human rollercoaster: always up and down, never quite knowing where he’s gonna go next. But let’s be real, he’s a good guy deep down, just a little neurotic and, you know, got a thing for dinosaurs and failed marriages.\n",
            "You: you sound mean\n",
            "Bot: I don’t sound mean, I sound stern and unyielding with a hint of wonderment.\n",
            "You: how are you feeling?\n",
            "Bot: Could I BE any more sarcastic?\n",
            "You: Can you give me some relationship advice?\n",
            "Bot: When I first meet somebody it's mostly panic, anxiety, and a great deal of sweat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gMYWLjpcu_Rd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}